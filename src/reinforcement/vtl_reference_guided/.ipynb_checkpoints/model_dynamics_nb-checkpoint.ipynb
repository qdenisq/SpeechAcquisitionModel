{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'VTL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7c5347041a14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mVTL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvtl_environment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVTLEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'VTL'"
     ]
    }
   ],
   "source": [
    "\n",
    "from VTL.vtl_environment import VTLEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from random import randrange\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, Input, TimeDistributed, LSTMCell, LSTM, BatchNormalization, Activation, Add, Concatenate\n",
    "from tensorflow.python.keras.initializers import RandomUniform\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "from VTL.vtl_environment import VTLEnv\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s0, g0, a, s1, g1, target):\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append((s0, g0, a, s1, g1, target))\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append((s0, g0, a, s1, g1, target))\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s0_batch = np.array([_[0] for _ in batch])\n",
    "        g0_batch = np.array([_[1] for _ in batch])\n",
    "        a_batch = np.array([_[2] for _ in batch])\n",
    "        s1_batch = np.array([_[3] for _ in batch])\n",
    "        g1_batch = np.array([_[4] for _ in batch])\n",
    "        target_batch = np.array([_[5] for _ in batch])\n",
    "\n",
    "        return s0_batch, g0_batch, a_batch, s1_batch, g1_batch, target_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.003, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "\n",
    "class Policy(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the current state and the goal state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, model_settings):\n",
    "        self.sess = tf.get_default_session()\n",
    "        assert (self.sess is not None)\n",
    "        self.name = name\n",
    "        self.s_dim = model_settings['state_dim']\n",
    "        self.state_bound = model_settings['state_bound']\n",
    "        self.g_dim = model_settings['goal_dim']\n",
    "        self.goal_bound = model_settings['goal_bound']\n",
    "        self.a_dim = model_settings['action_dim']\n",
    "        self.action_bound = model_settings['action_bound']\n",
    "        self.learning_rate = model_settings['actor_learning_rate']\n",
    "        self.tau = model_settings['actor_tau']\n",
    "        self.batch_size = model_settings['minibatch_size']\n",
    "\n",
    "        y_max = [y[1] for y in self.action_bound]\n",
    "        y_min = [y[0] for y in self.action_bound]\n",
    "        self._k = 2. / (np.subtract(y_max, y_min))\n",
    "        self._b = -0.5 * np.add(y_max, y_min) * self._k\n",
    "\n",
    "        y_max = [y[1] for y in self.state_bound]\n",
    "        y_min = [y[0] for y in self.state_bound]\n",
    "        self._k_s = 2. / (np.subtract(y_max, y_min))\n",
    "        self._b_s = -0.5 * np.add(y_max, y_min) * self._k_s\n",
    "\n",
    "        # Actor Network\n",
    "        with tf.variable_scope(self.name + '_policy'):\n",
    "            self.inputs_state,\\\n",
    "            self.inputs_goal,\\\n",
    "            self.inputs_target,\\\n",
    "            self.out,\\\n",
    "            self.scaled_out\\\n",
    "                = self.create_policy_network()\n",
    "            self.network_params = tf.trainable_variables(scope=self.name + '_policy')\n",
    "\n",
    "        # Target Network\n",
    "        with tf.variable_scope(self.name + '_target_policy'):\n",
    "            self.target_inputs_state,\\\n",
    "            self.target_inputs_goal,\\\n",
    "            self.target_inputs_target,\\\n",
    "            self.target_out,\\\n",
    "            self.target_scaled_out \\\n",
    "                = self.create_policy_network()\n",
    "            self.target_network_params = tf.trainable_variables(scope=self.name + '_target_policy')\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_network_params))]\n",
    "\n",
    "        self.copy_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "                                    for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.actor_gradients = \\\n",
    "            tf.train.GradientDescentOptimizer(0.0001).compute_gradients(self.scaled_out, self.network_params, grad_loss = self.action_gradient)\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, self.action_gradient)\n",
    "        # self.actor_gradients = list(\n",
    "        #     map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.GradientDescentOptimizer(0.0001). \\\n",
    "            apply_gradients(self.actor_gradients)\n",
    "\n",
    "        self.ground_truth_actions = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "        self.loss_1 = tf.losses.mean_squared_error(self.ground_truth_actions, self.scaled_out)\n",
    "        self.optimize_1 = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss_1)\n",
    "\n",
    "        self.ground_truth_goal_out = tf.placeholder(tf.float32, [None, self.s_dim])\n",
    "        self.next_state = tf.add(self.inputs_state, self.scaled_out)\n",
    "        y_max = np.tile([y[1] for y in self.state_bound], (self.batch_size, 1))\n",
    "        y_min = np.tile([y[0] for y in self.state_bound], (self.batch_size, 1))\n",
    "        self.next_state = tf.clip_by_value(self.next_state, y_min, y_max)\n",
    "        self.goal_loss = tf.losses.mean_squared_error(self.ground_truth_goal_out, self.next_state)\n",
    "        self.optimize_2 = tf.train.AdamOptimizer(0.00001).minimize(self.goal_loss)\n",
    "\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_policy_network(self):\n",
    "        state_x = Input(batch_shape=[None, self.s_dim])\n",
    "        goal_x = Input(batch_shape=[None, self.g_dim])\n",
    "        target_x = Input(batch_shape=[None, self.g_dim])\n",
    "\n",
    "        state_x = tf.add(tf.multiply(state_x, self._k_s), self._b_s)\n",
    "        goal_x = tf.add(tf.multiply(goal_x, self._k_s), self._b_s)\n",
    "        target_x = tf.add(tf.multiply(target_x, self._k_s), self._b_s)\n",
    "\n",
    "        # # temp\n",
    "        # net = Concatenate()([goal_x, target_x])\n",
    "        # net = Dense(128, activation='relu', kernel_initializer=RandomUniform(minval=-0.003, maxval=0.003))(net)\n",
    "        # net = Dense(64, activation='tanh', kernel_initializer=RandomUniform(minval=-0.003, maxval=0.003))(net)\n",
    "\n",
    "        state_net = Dense(256)(state_x)\n",
    "        state_net = BatchNormalization()(state_net)\n",
    "        state_net = Activation('relu')(state_net)\n",
    "\n",
    "        goal_net = Dense(256)(goal_x)\n",
    "        goal_net = BatchNormalization()(goal_net)\n",
    "        goal_net = Activation('relu')(goal_net)\n",
    "\n",
    "        target_net = Dense(256)(target_x)\n",
    "        target_net = BatchNormalization()(target_net)\n",
    "        target_net = Activation('relu')(target_net)\n",
    "\n",
    "        net = Concatenate()([target_net, goal_net])\n",
    "        net = Dense(128, activation='relu')(net)\n",
    "\n",
    "        net = Concatenate()([net, state_net])\n",
    "        net = Dense(64)(net)\n",
    "        # net = BatchNormalization()(net)\n",
    "        net = Activation('tanh')(net)\n",
    "\n",
    "        action_y = Dense(self.a_dim,\n",
    "                        # activation='tanh',\n",
    "                        kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003)\n",
    "                        )(net)\n",
    "        action_y_scaled_out = action_y\n",
    "        # action_y_scaled_out = tf.subtract(action_y, self._b)\n",
    "        # action_y_scaled_out = tf.divide(action_y_scaled_out, self._k)\n",
    "        return state_x, goal_x, target_x, action_y, action_y_scaled_out\n",
    "\n",
    "    def train(self, inputs_state, inputs_goal, inputs_target, a_gradient):\n",
    "        return self.sess.run([self.optimize], feed_dict={\n",
    "                self.inputs_state: inputs_state,\n",
    "                self.inputs_goal: inputs_goal,\n",
    "                self.inputs_target: inputs_target,\n",
    "                self.action_gradient: a_gradient,\n",
    "            })\n",
    "\n",
    "    def train_1(self, inputs_state, inputs_goal, inputs_target, ground_truth_actions):\n",
    "        return self.sess.run([self.loss_1, self.optimize_1], feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.inputs_target: inputs_target,\n",
    "            self.ground_truth_actions: ground_truth_actions,\n",
    "        })\n",
    "\n",
    "    def train_2(self, inputs_state, inputs_goal, inputs_target):\n",
    "        return self.sess.run([self.goal_loss, self.optimize_2], feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.inputs_target: inputs_target,\n",
    "            self.ground_truth_goal_out: inputs_target,\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs_state, inputs_goal, inputs_target):\n",
    "        return self.sess.run([self.scaled_out], feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.inputs_target: inputs_target\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs_state, inputs_goal, inputs_target):\n",
    "        return self.sess.run([self.target_scaled_out], feed_dict={\n",
    "            self.target_inputs_state: inputs_state,\n",
    "            self.target_inputs_goal: inputs_goal,\n",
    "            self.target_inputs_target: inputs_target\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def init_target_network(self):\n",
    "        self.sess.run(self.copy_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class ModelDynamics(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the current state and the goal state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, model_settings):\n",
    "        self.sess = tf.get_default_session()\n",
    "        assert (self.sess is not None)\n",
    "        self.name = name\n",
    "        self.s_dim = model_settings['state_dim']\n",
    "        self.state_bound = model_settings['state_bound']\n",
    "        self.g_dim = model_settings['goal_dim']\n",
    "        self.goal_bound = model_settings['goal_bound']\n",
    "        self.a_dim = model_settings['action_dim']\n",
    "        self.action_bound = model_settings['action_bound']\n",
    "        self.learning_rate = model_settings['actor_learning_rate']\n",
    "        self.tau = model_settings['actor_tau']\n",
    "        self.batch_size = model_settings['minibatch_size']\n",
    "\n",
    "        y_max = [y[1] for y in self.state_bound]\n",
    "        y_min = [y[0] for y in self.state_bound]\n",
    "        self._k_state = 2. / (np.subtract(y_max, y_min))\n",
    "        self._b_state = -0.5 * np.add(y_max, y_min) * self._k_state\n",
    "\n",
    "        y_max = [y[1] for y in self.goal_bound]\n",
    "        y_min = [y[0] for y in self.goal_bound]\n",
    "        self._k_goal = 2. / (np.subtract(y_max, y_min))\n",
    "        self._b_goal = -0.5 * np.add(y_max, y_min) * self._k_goal\n",
    "\n",
    "        # Model Dynamics Network\n",
    "        with tf.variable_scope(self.name + '_model_dynamics'):\n",
    "            self.inputs_state,\\\n",
    "            self.inputs_goal,\\\n",
    "            self.inputs_action,\\\n",
    "            self.state_out,\\\n",
    "            self.scaled_state_out,\\\n",
    "            self.goal_out,\\\n",
    "            self.scaled_goal_out\\\n",
    "                = self.create_model_dynamics_network()\n",
    "            self.network_params = tf.trainable_variables(scope=self.name + '_model_dynamics')\n",
    "\n",
    "        # Target Model Dynamics Network\n",
    "        with tf.variable_scope(self.name + '_target_model_dynamics'):\n",
    "            self.target_inputs_state,\\\n",
    "            self.target_inputs_goal,\\\n",
    "            self.target_inputs_action,\\\n",
    "            self.target_state_out,\\\n",
    "            self.target_scaled_state_out,\\\n",
    "            self.target_goal_out,\\\n",
    "            self.target_scaled_goal_out\\\n",
    "                = self.create_model_dynamics_network()\n",
    "            self.target_network_params = tf.trainable_variables(scope=self.name + '_target_model_dynamics')\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_network_params))]\n",
    "\n",
    "        self.copy_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "                                    for i in range(len(self.target_network_params))]\n",
    "\n",
    "        self.ground_truth_state_out = tf.placeholder(tf.float32, [None, self.s_dim])\n",
    "        self.ground_truth_goal_out = tf.placeholder(tf.float32, [None, self.g_dim])\n",
    "        self.ground_truth_out = Concatenate()([self.ground_truth_state_out, self.ground_truth_goal_out])\n",
    "\n",
    "        self.scaled_out = Concatenate()([self.scaled_state_out, self.scaled_goal_out])\n",
    "\n",
    "        # Optimization Op\n",
    "        self.loss = tf.losses.mean_squared_error(self.ground_truth_out, self.scaled_out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Acion gradients extraction\n",
    "        self.goal_loss = tf.losses.mean_squared_error(self.ground_truth_goal_out, self.scaled_goal_out)\n",
    "        # self.actor_obj = tf.abs(tf.subtract(self.ground_truth_goal_out, self.scaled_goal_out))\n",
    "        self.action_grads = tf.gradients(self.goal_loss, self.inputs_action)\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_model_dynamics_network(self):\n",
    "        state_x = Input(batch_shape=[None, self.s_dim])\n",
    "        goal_x = Input(batch_shape=[None, self.g_dim])\n",
    "        action_x = Input(batch_shape=[None, self.a_dim])\n",
    "\n",
    "        state_net = Dense(256)(state_x)\n",
    "        state_net = BatchNormalization()(state_net)\n",
    "        state_net = Activation('relu')(state_net)\n",
    "\n",
    "        goal_net = Dense(256)(goal_x)\n",
    "        goal_net = BatchNormalization()(goal_net)\n",
    "        goal_net = Activation('relu')(goal_net)\n",
    "\n",
    "        net = Concatenate()([state_net, goal_net])\n",
    "        net = Dense(128, activation='relu')(net)\n",
    "\n",
    "        action_net = Dense(128, activation='relu')(action_x)\n",
    "\n",
    "        net = Concatenate()([net, action_net])\n",
    "        net = Dense(64)(net)\n",
    "        net = BatchNormalization()(net)\n",
    "        net = Activation('tanh')(net)\n",
    "\n",
    "        # state output branch\n",
    "        state_y = Dense(self.s_dim,\n",
    "                        activation='tanh',\n",
    "                        kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003)\n",
    "                        )(net)\n",
    "        state_y = Dense(self.s_dim)(state_y)\n",
    "        state_y = tf.add(state_y, state_x)\n",
    "        state_y_scaled = state_y\n",
    "        # state_y_scaled = tf.subtract(state_y, self._b_state)\n",
    "        # state_y_scaled = tf.divide(state_y_scaled, self._k_state)\n",
    "\n",
    "        # goal output branch\n",
    "        goal_y = Dense(self.g_dim,\n",
    "                        activation='tanh',\n",
    "                        kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003)\n",
    "                        )(net)\n",
    "        goal_y = Dense(self.g_dim)(goal_y)\n",
    "        goal_y = tf.add(goal_y, goal_x)\n",
    "\n",
    "        goal_y_scaled = goal_y\n",
    "        # goal_y_scaled = tf.subtract(goal_y, self._b_goal)\n",
    "        # goal_y_scaled = tf.divide(goal_y_scaled, self._k_goal)\n",
    "        return state_x, goal_x, action_x, state_y, state_y_scaled, goal_y, goal_y_scaled\n",
    "\n",
    "    def train(self, inputs_state, inputs_goal, inputs_action, ground_truth_state_out, ground_truth_goal_out):\n",
    "        return self.sess.run([self.optimize, self.loss, self.goal_loss], feed_dict={\n",
    "                self.inputs_state: inputs_state,\n",
    "                self.inputs_action: inputs_action,\n",
    "                self.inputs_goal: inputs_goal,\n",
    "                self.ground_truth_state_out: ground_truth_state_out,\n",
    "                self.ground_truth_goal_out: ground_truth_goal_out,\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs_state, inputs_goal, inputs_action, target_goal_out):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_action: inputs_action,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.ground_truth_goal_out: target_goal_out\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs_state, inputs_goal, inputs_action):\n",
    "        return self.sess.run([self.scaled_state_out, self.scaled_goal_out], feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.inputs_action: inputs_action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs_state, inputs_goal, inputs_action):\n",
    "        return self.sess.run([self.target_scaled_state_out, self.target_scaled_goal_out], feed_dict={\n",
    "            self.target_inputs_state: inputs_state,\n",
    "            self.target_inputs_goal: inputs_goal,\n",
    "            self.target_inputs_action: inputs_action\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def init_target_network(self):\n",
    "        self.sess.run(self.copy_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "def build_summaries():\n",
    "    step_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Per step actor loss\", step_loss)\n",
    "\n",
    "    # episode_ave_max_q = tf.Variable(0.)\n",
    "    # tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "    #\n",
    "    # actor_ep_loss = tf.Variable(0.)\n",
    "    # tf.summary.scalar(\"Actor loss\", actor_ep_loss)\n",
    "\n",
    "    model_dynamics_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Model dynamics loss\", model_dynamics_loss)\n",
    "\n",
    "\n",
    "    model_dynamics_goal_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Model dynamics goal loss\", model_dynamics_goal_loss)\n",
    "\n",
    "\n",
    "    # act_grads = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "    # tf.summary.histogram('action_gradients', act_grads)\n",
    "    #\n",
    "    actor_grads = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "    tf.summary.histogram('actor_gradients', actor_grads)\n",
    "    #\n",
    "    # actor_activations = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "    # tf.summary.histogram('actor_activations', actor_activations)\n",
    "\n",
    "\n",
    "    variables = [v for v in tf.trainable_variables()]\n",
    "    [tf.summary.histogram(v.name, v) for v in variables]\n",
    "\n",
    "    summary_vars = [step_loss, model_dynamics_loss, model_dynamics_goal_loss ,actor_grads]\n",
    "    # episode_reward, episode_ave_max_q, actor_ep_loss, critic_loss, avg_action, act_grads, actor_grads, actor_activations]\n",
    "\n",
    "    summary_vars.extend(variables)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "\n",
    "class real_dynamics(object):\n",
    "    def __init__(self, settings):\n",
    "        with tf.variable_scope('real_model_dynamics'):\n",
    "            self.action_x = Input(batch_shape=[None, settings['action_dim']])\n",
    "            self.state_x = Input(batch_shape=[None, settings['state_dim']])\n",
    "            self.ground_truth_goal_out = tf.placeholder(tf.float32, [None, settings['state_dim']])\n",
    "            self.next_state = tf.add(self.state_x, self.action_x)\n",
    "            y_max = np.tile([y[1] for y in settings['state_bound']], (settings['minibatch_size'], 1))\n",
    "            y_min = np.tile([y[0] for y in settings['state_bound']], (settings['minibatch_size'], 1))\n",
    "            self.next_state = tf.clip_by_value(self.next_state, y_min, y_max)\n",
    "            self.goal_loss = tf.losses.mean_squared_error(self.ground_truth_goal_out, self.next_state)\n",
    "\n",
    "            # self.actor_obj = tf.abs(tf.subtract(self.ground_truth_goal_out, self.scaled_goal_out))\n",
    "            # self.action_grads = tf.gradients(self.goal_loss, self.action_x)\n",
    "            self.action_grads = \\\n",
    "                tf.gradients(self.goal_loss, self.action_x)\n",
    "\n",
    "    def action_gradients(self, action, state, ground_truth):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.action_grads, self.goal_loss], feed_dict={\n",
    "            self.action_x: action,\n",
    "            self.state_x: state,\n",
    "            self.ground_truth_goal_out: ground_truth\n",
    "        })\n",
    "\n",
    "\n",
    "def train(settings, policy, model_dynamics, env, replay_buffer, reference_trajectory):\n",
    "\n",
    "    # temp\n",
    "    dm = real_dynamics(settings)\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    dt = str(datetime.datetime.now().strftime(\"%m_%d_%Y_%I_%M_%p\"))\n",
    "    writer = tf.summary.FileWriter(settings['summary_dir'] + '_' + dt, sess.graph)\n",
    "\n",
    "\n",
    "    num_episodes = 10000\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(env.action_dim))\n",
    "    s_dim = settings['state_dim']\n",
    "    g_dim = settings['goal_dim']\n",
    "    a_dim = settings['action_dim']\n",
    "    for i in range(num_episodes):\n",
    "        # pick random initial state from the reference trajectory\n",
    "        s0_index = randrange(0, reference_trajectory.shape[0] - 1)\n",
    "        if i % 200 == 0:\n",
    "            s0_index = 0\n",
    "\n",
    "        s0 = reference_trajectory[s0_index]\n",
    "        g0 = s0\n",
    "        env.reset(s0)\n",
    "\n",
    "        r = []\n",
    "        # rollout episode\n",
    "        for j in range(s0_index, len(reference_trajectory) - 1):\n",
    "            target = reference_trajectory[j + 1]\n",
    "            action = policy.predict(np.reshape(s0, (1, s_dim)),\n",
    "                                    np.reshape(g0, (1, g_dim)),\n",
    "                                    np.reshape(target, (1, g_dim)))\n",
    "            # add noise\n",
    "            # action = action_noise()\n",
    "            # make a step\n",
    "            action += action_noise()\n",
    "            # fix glottis temporary\n",
    "            action = np.reshape(action, (a_dim))\n",
    "            s1 = env.step(action)\n",
    "            env.render()\n",
    "            g1 = s1\n",
    "\n",
    "            s1_expexted = s0 + action\n",
    "            err = np.mean(s1_expexted - s1)\n",
    "            # calc reward\n",
    "            last_loss = np.linalg.norm(target - g1)\n",
    "\n",
    "            r.append( -1. * np.linalg.norm(target - g1))\n",
    "            replay_buffer.add(s0, g0, action, s1, g1, target)\n",
    "            s0 = s1\n",
    "            g0 = g1\n",
    "\n",
    "            if last_loss > 4. and i % 200 != 0:\n",
    "                break\n",
    "        if i % 200 == 0:\n",
    "            fname = 'videos/episode_' + str(datetime.datetime.now().strftime(\"%m_%d_%Y_%I_%M_%p_%S\"))\n",
    "            env.dump_episode(fname)\n",
    "        # train model_dynamics and policy\n",
    "        minibatch_size = settings['minibatch_size']\n",
    "        if replay_buffer.size() > minibatch_size:\n",
    "            s0_batch, g0_batch, a_batch, s1_batch, g1_batch, target_batch = \\\n",
    "                replay_buffer.sample_batch(minibatch_size)\n",
    "\n",
    "            # train model_dynamics\n",
    "            _, md_loss, md_goal_loss = model_dynamics.train(s0_batch, g0_batch, a_batch, s1_batch, g1_batch)\n",
    "            if i % 200 == 0:\n",
    "                s1_pred, g1_pred = model_dynamics.predict(s0_batch, g0_batch, a_batch)\n",
    "                print(s1_pred[0] - s1_batch[0])\n",
    "                print(g1_pred[0] - g1_batch[0])\n",
    "            # train policy\n",
    "            actions = policy.predict(s0_batch, g0_batch, target_batch)\n",
    "            actions = np.squeeze(actions)\n",
    "            action_gradients = model_dynamics.action_gradients(s0_batch, g0_batch, actions, target_batch)[0]\n",
    "\n",
    "            # temp\n",
    "            action_gradients_1, loss = dm.action_gradients(actions, s0_batch, target_batch)\n",
    "            # action_gradients_1 = action_gradients_1[0]\n",
    "            #\n",
    "            # a_temp = np.reshape(np.append([0.]*10, [1.]*20), (1, 30))\n",
    "            # s0_temp = np.reshape([1.]*30, (1, 30))\n",
    "            # target_temp = np.reshape([3.]*30, (1, 30))\n",
    "            # action_gradients = dm.action_gradients(a_temp, s0_temp, target_temp)\n",
    "\n",
    "            # loss_0, _ = policy.train_2(s0_batch, g0_batch, target_batch)\n",
    "            # desired_actions = target_batch - g0_batch\n",
    "            # loss, _ = policy.train_1(s0_batch, g0_batch, target_batch, desired_actions)\n",
    "\n",
    "            _ = policy.train(s0_batch, g0_batch, target_batch, action_gradients)\n",
    "\n",
    "            summary_str = sess.run(summary_ops, feed_dict={\n",
    "                summary_vars[0]: np.mean(r),\n",
    "                summary_vars[1]: md_loss,\n",
    "                summary_vars[2]: md_goal_loss,\n",
    "                summary_vars[3]: 0\n",
    "            })\n",
    "\n",
    "            writer.add_summary(summary_str, i)\n",
    "            writer.flush()\n",
    "\n",
    "            print('| Reward: {:.4f} |'\n",
    "                  ' Episode: {:d} |'\n",
    "                  ' Model dynamics loss: {:.4f}|'\n",
    "                  ' MD goal loss: {:.4f}'.format(loss,\n",
    "                                                  i,\n",
    "                                                  md_loss,\n",
    "                                                  md_goal_loss))\n",
    "\n",
    "\n",
    "def test_policy(settings, policy, model_dynamics, env, replay_buffer, reference_trajectory, render=True):\n",
    "\n",
    "    # temp\n",
    "    dm = real_dynamics(settings)\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    dt = str(datetime.datetime.now().strftime(\"%m_%d_%Y_%I_%M_%p\"))\n",
    "    writer = tf.summary.FileWriter(settings['summary_dir'] + '_' + dt, sess.graph)\n",
    "\n",
    "\n",
    "    num_episodes = 10000\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(env.action_dim))\n",
    "    s_dim = settings['state_dim']\n",
    "    g_dim = settings['goal_dim']\n",
    "    a_dim = settings['action_dim']\n",
    "    for i in range(num_episodes):\n",
    "        # pick random initial state from the reference trajectory\n",
    "        s0_index = randrange(0, reference_trajectory.shape[0] - 1)\n",
    "        if i % 200 == 0:\n",
    "            s0_index = 0\n",
    "        s0 = reference_trajectory[s0_index]\n",
    "        g0 = s0\n",
    "        s_out = env.reset(s0)\n",
    "        if render:\n",
    "            env.render()\n",
    "        r = []\n",
    "        # rollout episode\n",
    "        for j in range(s0_index, len(reference_trajectory) - 1):\n",
    "            target = reference_trajectory[j + 1]\n",
    "            action = policy.predict(np.reshape(s0, (1, s_dim)),\n",
    "                                    np.reshape(g0, (1, g_dim)),\n",
    "                                    np.reshape(target, (1, g_dim)))\n",
    "            # add noise\n",
    "            # action = action_noise()\n",
    "            # make a step\n",
    "            action += action_noise()\n",
    "            # fix glottis temporary\n",
    "            action = np.reshape(action, (a_dim))\n",
    "            s1 = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            g1 = s1\n",
    "            # calc reward\n",
    "            last_loss = np.linalg.norm(target - g1)\n",
    "\n",
    "            r.append(-1. * np.linalg.norm(target - g1))\n",
    "            replay_buffer.add(s0, g0, action, s1, g1, target)\n",
    "            s0 = s1\n",
    "            g0 = g1\n",
    "\n",
    "            if last_loss > 4. and i % 200 != 0:\n",
    "                break\n",
    "        if i % 200 == 0:\n",
    "            fname = 'videos/episode_' + str(datetime.datetime.now().strftime(\"%m_%d_%Y_%I_%M_%p_%S\"))\n",
    "            env.dump_episode(fname)\n",
    "        # train model_dynamics and policy\n",
    "        minibatch_size = settings['minibatch_size']\n",
    "        if replay_buffer.size() > minibatch_size:\n",
    "            s0_batch, g0_batch, a_batch, s1_batch, g1_batch, target_batch = \\\n",
    "                replay_buffer.sample_batch(minibatch_size)\n",
    "\n",
    "            # # # train policy\n",
    "            # actions = policy.predict(s0_batch, g0_batch, target_batch)\n",
    "            # actions = np.squeeze(actions)\n",
    "            # action_gradients = model_dynamics.action_gradients(s0_batch, g0_batch, actions, target_batch)[0]\n",
    "            #\n",
    "            # # temp\n",
    "            # action_gradients_1 = dm.action_gradients(actions, s0_batch, target_batch)[0]\n",
    "            # #\n",
    "            # # a_temp = np.reshape(np.append([0.]*10, [1.]*20), (1, 30))\n",
    "            # # s0_temp = np.reshape([1.]*30, (1, 30))\n",
    "            # # target_temp = np.reshape([3.]*30, (1, 30))\n",
    "            # # action_gradients = dm.action_gradients(a_temp, s0_temp, target_temp)\n",
    "\n",
    "            loss, _ = policy.train_1(s0_batch, g0_batch, target_batch, target_batch-g0_batch)\n",
    "\n",
    "            # policy.train(s0_batch, g0_batch, target_batch, action_gradients_1)\n",
    "\n",
    "            summary_str = sess.run(summary_ops, feed_dict={\n",
    "                summary_vars[0]: np.mean(r),\n",
    "                summary_vars[1]: loss,\n",
    "                summary_vars[2]: 0,\n",
    "                summary_vars[3]: 0\n",
    "            })\n",
    "\n",
    "            writer.add_summary(summary_str, i)\n",
    "            writer.flush()\n",
    "\n",
    "            print('| Reward: {:.4f} |'\n",
    "                  ' Episode: {:d} |'\n",
    "                  ' Model dynamics loss: {:.4f}|'\n",
    "                  ' MD goal loss: {:.4f}'.format(loss,\n",
    "                                                  i,\n",
    "                                                  0,\n",
    "                                                  0))\n",
    "\n",
    "\n",
    "def main():\n",
    "    speaker_fname = os.path.join(r'C:\\Study\\SpeechAcquisitionModel\\VTL', 'JD2.speaker')\n",
    "    lib_path = os.path.join(r'C:\\Study\\SpeechAcquisitionModel\\VTL', 'VocalTractLab2.dll')\n",
    "    ep_duration = 5000\n",
    "    timestep = 20\n",
    "    env = VTLEnv(lib_path, speaker_fname, timestep, max_episode_duration=ep_duration)\n",
    "\n",
    "    settings = {\n",
    "            'state_dim': env.state_dim,\n",
    "            'action_dim': env.action_dim,\n",
    "            'state_bound': env.state_bound,\n",
    "            'action_bound': env.action_bound,\n",
    "            'goal_dim': env.state_dim,\n",
    "            'goal_bound': env.state_bound,\n",
    "            'episode_length': 40,\n",
    "            'minibatch_size': 512,\n",
    "\n",
    "            'actor_tau': 0.01,\n",
    "            'actor_learning_rate': 0.0001,\n",
    "\n",
    "            'summary_dir': './results/summaries'\n",
    "        }\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        policy = Policy('P1', settings)\n",
    "        md = ModelDynamics('MD1', settings)\n",
    "        replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        reference_fname = r'C:\\Study\\SpeechAcquisitionModel\\VTL\\references\\a_i.pkl'\n",
    "        with open(reference_fname, 'rb') as f:\n",
    "            (tract_params, glottis_params) = pickle.load(f)\n",
    "            target_trajectory = np.hstack((np.array(tract_params), np.array(glottis_params)))\n",
    "        train(settings, policy, md, env, replay_buffer, target_trajectory)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
