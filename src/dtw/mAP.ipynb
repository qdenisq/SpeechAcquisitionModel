{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Study\\SpeechAcquisitionModel\\src\\dtw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Study\\SpeechAcquisitionModel')\n",
    "print(os.getcwd())\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, minkowski\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "import dtw\n",
    "import dtwalign\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "from src.speech_classification.audio_processing import SpeechCommandsDataCollector, AudioPreprocessorMFCCDeltaDelta\n",
    "from src.siamese_net_sound_similarity.slstm_train import SiameseSpeechCommandsDataCollector\n",
    "from src.siamese_net_sound_similarity.train_v2_with_settle_structure import SiameseDeepLSTMNet\n",
    "from src.siamese_net_sound_similarity.train import SiameseLSTMNet\n",
    "\n",
    "from src.siamese_net_sound_similarity.soft_dtw import SoftDTW\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set(font_scale=1.4, rc={'text.usetex' : False})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE='euclidean'\n",
    "STEP_PATTERN = 'typeIc'\n",
    "CLOSED_END_STEP_PATTERN = \"typeIc\"\n",
    "# DISTANCE=lambda x, y : minkowski(x,y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_norm': True,\n",
      " 'dct_coefficient_count': 13,\n",
      " 'dist': 'l1',\n",
      " 'dropout': 0.3,\n",
      " 'hidden_fc': [1024, 1024, 1024],\n",
      " 'hidden_reccurent': [512, 512, 512],\n",
      " 'label_count': 32,\n",
      " 'learning_rate': 0.0005,\n",
      " 'loss_type': 'sdtw',\n",
      " 'margin': 0.4,\n",
      " 'mfcc_num': 39,\n",
      " 'mini_batch_size': 64,\n",
      " 'open_end': False,\n",
      " 'save_dir': \"r'C:\\\\Study\\\\SpeechAcquisitionModel\\\\models\\\\siamese_net_sound_similarity'\",\n",
      " 'train_steps': 100000,\n",
      " 'triplet_anneal_b': 1500,\n",
      " 'triplet_anneal_k': 0.0025,\n",
      " 'wanted_words': ['bed',\n",
      "                  'bird',\n",
      "                  'cat',\n",
      "                  'dog',\n",
      "                  'down',\n",
      "                  'eight',\n",
      "                  'five',\n",
      "                  'four',\n",
      "                  'go',\n",
      "                  'happy',\n",
      "                  'house',\n",
      "                  'left',\n",
      "                  'marvin',\n",
      "                  'nine',\n",
      "                  'no',\n",
      "                  'off',\n",
      "                  'on',\n",
      "                  'one',\n",
      "                  'right',\n",
      "                  'seven',\n",
      "                  'sheila',\n",
      "                  'six',\n",
      "                  'stop',\n",
      "                  'three',\n",
      "                  'tree',\n",
      "                  'two',\n",
      "                  'up',\n",
      "                  'wow',\n",
      "                  'yes',\n",
      "                  'zero'],\n",
      " 'winlen': 0.02,\n",
      " 'winstep': 0.01}\n"
     ]
    }
   ],
   "source": [
    "config_path = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_09_2020_05_52_PM\\config.yaml'\n",
    "with open(config_path, 'r') as data_file:\n",
    "    config = yaml.safe_load(data_file)\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = config['wanted_words']\n",
    "\n",
    "wanted_words_combined = wanted_words\n",
    "\n",
    "model_settings = config\n",
    "query_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = AudioPreprocessorMFCCDeltaDelta(numcep=model_settings['dct_coefficient_count'], winlen=model_settings['winlen'],\n",
    "                                     winstep=model_settings['winstep'])\n",
    "\n",
    "data_iter = SiameseSpeechCommandsDataCollector(preproc,\n",
    "                                        data_dir=r'C:\\Study\\Speech_command_classification\\speech_dataset',\n",
    "                                        wanted_words=wanted_words_combined,\n",
    "                                        testing_percentage=10,\n",
    "                                        validation_percentage=10\n",
    "                                        )\n",
    "\n",
    "index_to_word = {v:k for k,v in data_iter.word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "[17. 23.  9. ... 26.  9. 15.]\n",
      "[15. 10.  7. ... 10. 25. 30.]\n",
      "138\n",
      "(4000, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "n_mini_batch_size = 4000\n",
    "\n",
    "# gather data\n",
    "\n",
    "\n",
    "data = data_iter.get_data(n_mini_batch_size, 0, 'validation')\n",
    "labels = data['y']\n",
    "\n",
    "data1 = data_iter.get_data(n_mini_batch_size, 0, 'validation')\n",
    "labels1 = data1['y']\n",
    "idx = np.random.randint(0, len(labels1), len(labels1))\n",
    "data1_x = data1['x'][idx]\n",
    "data1['x'] = data1_x\n",
    "labels1 =  np.array(data1['y'])[idx]\n",
    "data1['y'] = labels1\n",
    "\n",
    "# labesl1 = data1['y']\n",
    "\n",
    "# duplicates = data_iter.get_duplicates(labels, 0, 'testing')\n",
    "# assert np.any(labels == duplicates['y'])\n",
    "# duplicates['x'] = duplicates['x'][:n_duplicates_size]\n",
    "# print(duplicates['x'].shape)\n",
    "\n",
    "# non_duplicates = data_iter.get_nonduplicates(labels, 0, 'testing')\n",
    "# assert np.any(labels != non_duplicates['y'])\n",
    "\n",
    "# y_true = np.concatenate((np.ones((n_duplicates_size)), np.zeros((n_mini_batch_size))))\n",
    "y_true = labels==labels1\n",
    "print(y_true)\n",
    "print(labels)\n",
    "print(labels1)\n",
    "print(sum(y_true))\n",
    "print(data['x'].shape)\n",
    "dists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MFCC Precision/Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize dist_lists\n",
    "\n",
    "dtw = []\n",
    "\n",
    "for i in range(n_mini_batch_size):\n",
    "    print(i, end='\\r')\n",
    "    x = data['x'][i].squeeze()\n",
    "    y = data1['x'][i].squeeze()\n",
    "    dtw.append(dtwalign.dtw(x, y, dist=DISTANCE, step_pattern=STEP_PATTERN, open_end=False, dist_only=True).normalized_distance)\n",
    "\n",
    "mfcc_dtw_dist = np.array(dtw)\n",
    "dists.append(mfcc_dtw_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Improving DTW using latent variables from LSTM classifier as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nn\n",
    "nn_ce = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_09_2020_05_52_PM\\net_43000_0.921875.net'\n",
    "nn_ce_l2 = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_15_2020_04_09_PM\\net_2000_0.9453125.net'\n",
    "nn_ce_dtw = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_13_2020_03_27_PM\\net_49000_0.9140625.net'\n",
    "nn_ce_cos = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_15_2020_03_09_PM\\net_1981_0.9453125.net'\n",
    "\n",
    "nn_dtw = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_17_2020_08_47_PM\\net_44000_0.0546875.net'\n",
    "nn_cos_hinge = r'C:\\Study\\SpeechAcquisitionModel\\reports\\seamise_net_01_17_2020_04_00_PM\\net_30500_0.046875.net'\n",
    "\n",
    "# net_237_0.10546875\n",
    "\n",
    "nn_fnames = [nn_ce_l2, nn_ce_cos, nn_ce_dtw, nn_dtw, nn_cos_hinge]\n",
    "nn_names = ['CE+L2', 'CE+COS_HINGE', 'CE+DTW', 'DTW', 'COS_HINGE']\n",
    "\n",
    "\n",
    "nn_fnames = [nn_cos_hinge, nn_dtw]\n",
    "nn_names = [\"COS_HINGE\", 'DTW']\n",
    "\n",
    "model_names = ['MFCC']\n",
    "model_names.extend(nn_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - SOFT-DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:453: SourceChangeWarning: source code of class 'src.siamese_net_sound_similarity.train_v2_with_settle_structure.SiameseDeepLSTMNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################################\n",
      "# Model COS_HINGE\n",
      "############################################################################\n",
      "############################################################################\n",
      "# PREFIX LENGTH : 100\n",
      "############################################################################\n",
      "############################################################################\n",
      "# Model DTW\n",
      "############################################################################\n",
      "############################################################################\n",
      "# PREFIX LENGTH : 100\n",
      "############################################################################\n",
      "0\r"
     ]
    }
   ],
   "source": [
    "\n",
    "soft_dtw_loss_close_end = SoftDTW(open_end=False, dist='l1')\n",
    "dist_funcs = {\n",
    "    'sdtw': soft_dtw_loss_close_end,\n",
    "    'l2': lambda x, y: torch.sum((x - y)**2, dim=-1)[-1],\n",
    "    'cos_hinge': lambda x, y: 1 - torch.nn.CosineSimilarity(dim=0)(x[-1, :], y[-1, :]),\n",
    "    'ce': soft_dtw_loss_close_end\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(nn_fnames)):\n",
    "    nn_fname = nn_fnames[i]\n",
    "    config_path = os.path.join(os.path.dirname(nn_fname), 'config.yaml')\n",
    "    with open(config_path, 'r') as data_file:\n",
    "        config = yaml.safe_load(data_file)\n",
    "   \n",
    "    loss_type = config['loss_type']\n",
    "    dist_func = dist_funcs[loss_type]\n",
    "    nn = torch.load(nn_fname).cpu()\n",
    "    nn.eval()\n",
    "\n",
    "    print(f\"############################################################################\\n\\\n",
    "# Model {model_names[i+1]}\\n\\\n",
    "############################################################################\")\n",
    "\n",
    "    PREFIX_LENS = [100]\n",
    "\n",
    "    for PREFIX_LEN in PREFIX_LENS:\n",
    "        print(f\"############################################################################\\n\\\n",
    "# PREFIX LENGTH : {PREFIX_LEN}\\n\\\n",
    "############################################################################\")\n",
    "        nn_input = torch.from_numpy(data['x'][:, :PREFIX_LEN, :]).float()\n",
    "\n",
    "\n",
    "        z, *_ = nn.single_forward(nn_input)\n",
    "        z = z\n",
    "\n",
    "        z1, *_ = nn.single_forward(torch.from_numpy(data1['x']).float())\n",
    "        z1 = z1\n",
    "\n",
    "\n",
    "        ###########################################################################\n",
    "        # DTW\n",
    "        ###########################################################################\n",
    "\n",
    "#         soft_dtw_loss_open_end = SoftDTW(open_end=True, dist='l1')\n",
    "#         soft_dtw_loss_close_end = SoftDTW(open_end=False, dist='l1')\n",
    "\n",
    "        ##########################################################################\n",
    "        # OPEN END = False\n",
    "        ##########################################################################\n",
    "\n",
    "        # initialize dist_lists\n",
    "\n",
    "        dtw_z = []\n",
    "\n",
    "        for i in range(n_mini_batch_size):\n",
    "            print(i, end='\\r')\n",
    "            x = z[i].squeeze()\n",
    "            y = z1[i].squeeze()\n",
    "\n",
    "            d = dist_func(x, y)\n",
    "            dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "        dtw_dist = np.array(dtw_z)\n",
    "        dists.append(dtw_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Precision_Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# no skill model\n",
    "precision, recall, _ = precision_recall_curve(y_true, np.zeros(len(y_true)), pos_label=1)\n",
    "ax = sns.lineplot(recall, precision)\n",
    "ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, -dists[i], pos_label=1)\n",
    "    print(model_names[i])\n",
    "    if model_names[i].startswith('CE'):\n",
    "        st = '--'\n",
    "    else:\n",
    "        st = '-'\n",
    "    print(st)\n",
    "    sns.lineplot(recall, precision, label=model_names[i], linewidth=3, dashes=st)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Average Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_names)):\n",
    "    average_precision = average_precision_score(y_true, -dists[i])\n",
    "    print(model_names[i], average_precision)\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, models_dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
