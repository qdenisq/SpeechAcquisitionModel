{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "import dtw\n",
    "import dtwalign\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "from src.speech_classification.audio_processing import AudioPreprocessorFbank, SpeechCommandsDataCollector\n",
    "from src.siamese_net_sound_similarity.slstm_train import SiameseSpeechCommandsDataCollector, StochasticSiameseLSTMNet\n",
    "from src.siamese_net_sound_similarity.train import SiameseLSTMNet\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE='canberra'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = ['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left',\n",
    "                'marvin',\n",
    "                'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two',\n",
    "                'up', 'wow', 'yes', 'zero']\n",
    "\n",
    "wanted_words_combined = wanted_words\n",
    "\n",
    "model_settings = {\n",
    "    'dct_coefficient_count': 26,\n",
    "    'label_count': len(wanted_words_combined) + 2,\n",
    "    'hidden_reccurent_cells_count': 128,\n",
    "    'winlen': 0.04,\n",
    "    'winstep': 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = AudioPreprocessorFbank(nfilt=model_settings['dct_coefficient_count'], winlen=model_settings['winlen'],\n",
    "                                     winstep=model_settings['winstep'])\n",
    "\n",
    "data_iter = SiameseSpeechCommandsDataCollector(preproc,\n",
    "                                        data_dir=r'C:\\Study\\Speech_command_classification\\speech_dataset',\n",
    "                                        wanted_words=wanted_words_combined,\n",
    "                                        testing_percentage=10,\n",
    "                                        validation_percentage=10\n",
    "                                        )\n",
    "\n",
    "index_to_word = {v:k for k,v in data_iter.word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mini_batch_size = 1\n",
    "data = data_iter.get_data(n_mini_batch_size, 0, 'training')\n",
    "labels = data['y']\n",
    "\n",
    "duplicates = data_iter.get_duplicates(labels, 0, 'training')\n",
    "assert np.any(labels == duplicates['y'])\n",
    "\n",
    "non_duplicates = data_iter.get_nonduplicates(labels, 0, 'training')\n",
    "assert np.any(labels != non_duplicates['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = np.random.randint(data['y'].shape[0])\n",
    "sample_idx = i\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15,10))\n",
    "\n",
    "# plt.figure()\n",
    "im = axes[0][0].imshow(data['x'][i].T)\n",
    "# fig.colorbar(im)\n",
    "label = index_to_word[data['y'][0]]\n",
    "axes[0][0].title.set_text(f'{label}')\n",
    "\n",
    "axes[0][1].imshow(duplicates['x'][i].T)\n",
    "# plt.colorbar()\n",
    "label = index_to_word[data['y'][0]]\n",
    "axes[0][1].title.set_text(f'{label}')\n",
    "\n",
    "\n",
    "axes[0][2].imshow(non_duplicates['x'][i].T)\n",
    "# plt.colorbar()\n",
    "label = index_to_word[non_duplicates['y'][0]]\n",
    "axes[0][2].title.set_text(f'{label}')\n",
    "\n",
    "######################################## DTW\n",
    "\n",
    "x = data['x'][i]\n",
    "\n",
    "# self dtw (extpected to be 0)\n",
    "\n",
    "y = data['x'][i]\n",
    "res = dtwalign.dtw(x, y, dist=DISTANCE, open_end=True)\n",
    "res.plot_path(ax=axes[1][0])\n",
    "axes[1][0].set_title(f\"anchor DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "\n",
    "# positive dtw\n",
    "\n",
    "y = duplicates['x'][sample_idx]\n",
    "res = dtwalign.dtw(x, y, dist=DISTANCE, open_end=True)\n",
    "res.plot_path(ax=axes[1][1])\n",
    "axes[1][1].set_title(f\"positive DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "\n",
    "# negative dtw\n",
    "\n",
    "y = non_duplicates['x'][sample_idx]\n",
    "res = dtwalign.dtw(x, y, dist=DISTANCE, open_end=True)\n",
    "res.plot_path(ax=axes[1][2])\n",
    "axes[1][2].set_title(f\"negative DTW = {res.normalized_distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mini_batch_size = 3000\n",
    "\n",
    "# gather data\n",
    "\n",
    "\n",
    "data = data_iter.get_data(n_mini_batch_size, 0, 'training')\n",
    "labels = data['y']\n",
    "\n",
    "duplicates = data_iter.get_duplicates(labels, 0, 'training')\n",
    "assert np.any(labels == duplicates['y'])\n",
    "\n",
    "non_duplicates = data_iter.get_nonduplicates(labels, 0, 'training')\n",
    "assert np.any(labels != non_duplicates['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW distribution on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize dist_lists\n",
    "\n",
    "duplicates_dtw = []\n",
    "non_duplicates_dtw = []\n",
    "\n",
    "for i in range(n_mini_batch_size):\n",
    "    print(i, end='\\r')\n",
    "    x = data['x'][i].squeeze()\n",
    "    \n",
    "    y_duplicate = duplicates['x'][i].squeeze()\n",
    "    duplicates_dtw.append(dtwalign.dtw(x, y_duplicate, dist=DISTANCE, open_end=False, dist_only=True).normalized_distance)\n",
    "\n",
    "    y_non_duplicate = non_duplicates['x'][i].squeeze()\n",
    "    non_duplicates_dtw.append(dtwalign.dtw(x, y_non_duplicate, dist=DISTANCE, open_end=False, dist_only=True).normalized_distance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 100., 100)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15,10))\n",
    "\n",
    "duplicates_dtw_filtered = np.array(duplicates_dtw)[np.where(np.array(duplicates_dtw) < 100.)]\n",
    "non_duplicates_dtw_filtered = np.array(non_duplicates_dtw)[np.where(np.array(non_duplicates_dtw) < 100.)]\n",
    "\n",
    "\n",
    "sns.distplot(duplicates_dtw,  bins=bins, ax=axes[0], kde=False)\n",
    "axes[0].title.set_text('duplicates DTW')\n",
    "\n",
    "sns.distplot(non_duplicates_dtw, bins=bins,  color='y', ax=axes[1], kde=False)\n",
    "axes[1].title.set_text('non-duplicates DTW')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence between two DTW distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dup, _ = np.histogram(duplicates_dtw)\n",
    "hist_dup = hist_dup / np.sum(hist_dup)\n",
    "\n",
    "hist_non_dup, _ = np.histogram(non_duplicates_dtw)\n",
    "hist_non_dup = hist_non_dup / np.sum(hist_non_dup)\n",
    "\n",
    "dkl = scipy.special.kl_div(hist_dup+1e-5, hist_non_dup+1e-5).sum()\n",
    "print(f\"DKL is: {dkl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As it can be seen, DTW calculated on MFCCs signals failed to capture different and similar samples showing no significant difference between duplicates and non-duplicates pairs. Kullback-Leibler divergence just proves this fact that P (distribution of distances between duplicate samples) and Q (non-duplicate ones) have no significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving DTW using latent variables from LSTM classifier as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nn\n",
    "\n",
    "nn_fname = r'C:/Study/SpeechAcquisitionModel/reports/seamise_net_09_25_2019_03_11_PM/net_0.5.net'\n",
    "\n",
    "nn = torch.load(nn_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PREFIX_LENS = [50, 40, 30, 20]\n",
    "\n",
    "for PREFIX_LEN in PREFIX_LENS:\n",
    "    nn_input = torch.from_numpy(data['x'][:, :PREFIX_LEN, :]).cuda().float()\n",
    "\n",
    "\n",
    "    z, *_ = nn.single_forward(nn_input)\n",
    "    z = z.detach().cpu().numpy()\n",
    "\n",
    "    duplicates_z, *_ = nn.single_forward(torch.from_numpy(duplicates['x']).cuda().float())\n",
    "    duplicates_z = duplicates_z.detach().cpu().numpy()\n",
    "\n",
    "    non_duplicates_z, *_ = nn.single_forward(torch.from_numpy(non_duplicates['x']).cuda().float())\n",
    "    non_duplicates_z = non_duplicates_z.detach().cpu().numpy()\n",
    "\n",
    "    #########################################################################\n",
    "    # Show example\n",
    "    #########################################################################\n",
    "\n",
    "    i = np.random.randint(data['y'].shape[0])\n",
    "    sample_idx = i\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15,10))\n",
    "\n",
    "    # plt.figure()\n",
    "    im = axes[0][0].imshow(z[i].T, vmin=-1, vmax=1)\n",
    "    # fig.colorbar(im)\n",
    "    label = index_to_word[data['y'][i]]\n",
    "    axes[0][0].title.set_text(f'{label}')\n",
    "\n",
    "    axes[0][1].imshow(duplicates_z[i].T, vmin=-1, vmax=1)\n",
    "    # plt.colorbar()\n",
    "    label = index_to_word[data['y'][i]]\n",
    "    axes[0][1].title.set_text(f'{label}')\n",
    "\n",
    "\n",
    "    axes[0][2].imshow(non_duplicates_z[i].T, vmin=-1, vmax=1)\n",
    "    # plt.colorbar()\n",
    "    label = index_to_word[non_duplicates['y'][i]]\n",
    "    axes[0][2].title.set_text(f'{label}')\n",
    "\n",
    "    # DTW\n",
    "\n",
    "    x = z[sample_idx]\n",
    "\n",
    "    # self dtw (extpected to be 0)\n",
    "\n",
    "    y = z[sample_idx]\n",
    "    res = dtwalign.dtw(x, y, dist=DISTANCE, open_end=True)\n",
    "    res.plot_path(ax=axes[1][0])\n",
    "    axes[1][0].set_title(f\"anchor DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "\n",
    "    # positive dtw\n",
    "\n",
    "    y = duplicates_z[sample_idx]\n",
    "    res = dtwalign.dtw(x, y, dist=DISTANCE, open_end=True)\n",
    "    res.plot_path(ax=axes[1][1])\n",
    "    axes[1][1].set_title(f\"positive DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "\n",
    "    # negative dtw\n",
    "\n",
    "    y = non_duplicates_z[sample_idx]\n",
    "    res = dtwalign.dtw(x, y, dist=DISTANCE, open_end=True)\n",
    "    res.plot_path(ax=axes[1][2])\n",
    "    axes[1][2].set_title(f\"negative DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ###########################################################################\n",
    "    # DTW\n",
    "    ###########################################################################\n",
    "\n",
    "    ##########################################################################\n",
    "    #OPEN END = TRUE\n",
    "    ##########################################################################\n",
    "\n",
    "    # initialize dist_lists\n",
    "\n",
    "    duplicates_dtw_z = []\n",
    "    non_duplicates_dtw_z = []\n",
    "\n",
    "    duplicates_bmis = []\n",
    "    non_duplicates_bmis = []\n",
    "\n",
    "    for i in range(n_mini_batch_size):\n",
    "        print(i, end='\\r')\n",
    "        x = z[i].squeeze()\n",
    "\n",
    "        y_duplicate = duplicates_z[i].squeeze()\n",
    "        d = dtwalign.dtw(x, y_duplicate, dist=DISTANCE, open_end=True)\n",
    "        duplicates_dtw_z.append(d.normalized_distance)\n",
    "        duplicates_bmis.append(d.path[-1, -1])\n",
    "\n",
    "        y_non_duplicate = non_duplicates_z[i].squeeze()\n",
    "        d = dtwalign.dtw(x, y_non_duplicate, dist=DISTANCE, open_end=True)\n",
    "        non_duplicates_dtw_z.append(d.normalized_distance)\n",
    "        non_duplicates_bmis.append(d.path[-1, -1])\n",
    "\n",
    "    # Show DTW\n",
    "\n",
    "    bins = np.linspace(min(np.min(duplicates_dtw_z), np.min(non_duplicates_dtw_z)),\n",
    "                       max(np.max(duplicates_dtw_z), np.max(non_duplicates_dtw_z)),\n",
    "                       100)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15,8))\n",
    "\n",
    "    duplicates_bmis = np.array(duplicates_bmis)\n",
    "    non_duplicates_bmis = np.array(non_duplicates_bmis)\n",
    "\n",
    "\n",
    "    sns.distplot(duplicates_dtw_z, bins=bins, ax=axes, label=\"Positives\")\n",
    "    axes.title.set_text(f'DTW (open_end = True) | prefix_len={PREFIX_LEN}')\n",
    "\n",
    "    sns.distplot(non_duplicates_dtw_z, bins=bins, color='y', ax=axes, label='Negatives')\n",
    "    axes.legend()\n",
    "    # axes[1].title.set_text('non-duplicates DTW')\n",
    "    plt.show()\n",
    "\n",
    "    hist_dup, _ = np.histogram(duplicates_dtw_z, bins=bins)\n",
    "    hist_dup = hist_dup / np.sum(hist_dup)\n",
    "\n",
    "    hist_non_dup, _ = np.histogram(non_duplicates_dtw_z, bins=bins)\n",
    "    hist_non_dup = hist_non_dup / np.sum(hist_non_dup)\n",
    "\n",
    "    dkl = scipy.special.kl_div(hist_dup+1e-5, hist_non_dup+1e-5).sum()\n",
    "    print(f\"DKL is: {dkl:.4f}\")\n",
    "\n",
    "    # plot matching segment length\n",
    "\n",
    "    bins = np.linspace(0, 50., 50)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15,8))\n",
    "\n",
    "    sns.distplot(duplicates_bmis, ax=axes,  label=\"Positives\", kde=False, bins=bins)\n",
    "    axes.title.set_text(f'Matching segment length | prefix_len={PREFIX_LEN}')\n",
    "\n",
    "    sns.distplot(non_duplicates_bmis,   color='y', ax=axes, label='Negatives', kde=False, bins=bins)\n",
    "    axes.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ##########################################################################\n",
    "    # OPEN END = False\n",
    "    ##########################################################################\n",
    "\n",
    "    # initialize dist_lists\n",
    "\n",
    "    duplicates_dtw_z = []\n",
    "    non_duplicates_dtw_z = []\n",
    "\n",
    "    duplicates_bmis = []\n",
    "    non_duplicates_bmis = []\n",
    "\n",
    "    for i in range(n_mini_batch_size):\n",
    "        print(i, end='\\r')\n",
    "        x = z[i].squeeze()\n",
    "\n",
    "        y_duplicate = duplicates_z[i].squeeze()\n",
    "        d = dtwalign.dtw(x, y_duplicate, dist=DISTANCE, open_end=False)\n",
    "        duplicates_dtw_z.append(d.normalized_distance)\n",
    "        duplicates_bmis.append(d.path[-1, -1])\n",
    "\n",
    "        y_non_duplicate = non_duplicates_z[i].squeeze()\n",
    "        d = dtwalign.dtw(x, y_non_duplicate, dist=DISTANCE, open_end=False)\n",
    "        non_duplicates_dtw_z.append(d.normalized_distance)\n",
    "        non_duplicates_bmis.append(d.path[-1, -1])\n",
    "\n",
    "\n",
    "    bins = np.linspace(min(np.min(duplicates_dtw_z), np.min(non_duplicates_dtw_z)),\n",
    "                       max(np.max(duplicates_dtw_z), np.max(non_duplicates_dtw_z)),\n",
    "                       100)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15,8))\n",
    "\n",
    "    sns.distplot(duplicates_dtw_z, bins=bins, ax=axes, label=\"Positives\")\n",
    "    axes.title.set_text(f'DTW (open_end = False) | prefix_len={PREFIX_LEN}')\n",
    " \n",
    "    sns.distplot(non_duplicates_dtw_z, bins=bins, color='y', ax=axes, label='Negatives')\n",
    "    axes.legend()\n",
    "    # axes[1].title.set_text('non-duplicates DTW')\n",
    "    plt.show()\n",
    "\n",
    "    hist_dup, _ = np.histogram(duplicates_dtw_z, bins=bins)\n",
    "    hist_dup = hist_dup / np.sum(hist_dup)\n",
    "\n",
    "    hist_non_dup, _ = np.histogram(non_duplicates_dtw_z, bins=bins)\n",
    "    hist_non_dup = hist_non_dup / np.sum(hist_non_dup)\n",
    "\n",
    "    dkl = scipy.special.kl_div(hist_dup+1e-5, hist_non_dup+1e-5).sum()\n",
    "    print(f\"DKL is: {dkl:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
