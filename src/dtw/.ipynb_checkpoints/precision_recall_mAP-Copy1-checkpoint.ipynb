{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Study\\SpeechAcquisitionModel\\src\\dtw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Study\\SpeechAcquisitionModel')\n",
    "print(os.getcwd())\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, minkowski\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "import dtw\n",
    "import dtwalign\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "from src.speech_classification.audio_processing import AudioPreprocessorFbank, SpeechCommandsDataCollector\n",
    "from src.siamese_net_sound_similarity.slstm_train import SiameseSpeechCommandsDataCollector, StochasticSiameseLSTMNet\n",
    "from src.siamese_net_sound_similarity.train import SiameseLSTMNet\n",
    "from src.siamese_net_sound_similarity.soft_dtw import SoftDTW\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set(font_scale=1.4, rc={'text.usetex' : False})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE='euclidean'\n",
    "STEP_PATTERN = 'typeIc'\n",
    "CLOSED_END_STEP_PATTERN = \"typeIc\"\n",
    "# DISTANCE=lambda x, y : minkowski(x,y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = ['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left',\n",
    "                'marvin',\n",
    "                'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two',\n",
    "                'up', 'wow', 'yes', 'zero']\n",
    "\n",
    "wanted_words_combined = wanted_words\n",
    "\n",
    "model_settings = {\n",
    "    'dct_coefficient_count': 26,\n",
    "    'label_count': len(wanted_words_combined) + 2,\n",
    "    'hidden_reccurent_cells_count': 128,\n",
    "    'winlen': 0.04,\n",
    "    'winstep': 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = AudioPreprocessorFbank(nfilt=model_settings['dct_coefficient_count'], winlen=model_settings['winlen'],\n",
    "                                     winstep=model_settings['winstep'])\n",
    "\n",
    "data_iter = SiameseSpeechCommandsDataCollector(preproc,\n",
    "                                        data_dir=r'C:\\Study\\Speech_command_classification\\speech_dataset',\n",
    "                                        wanted_words=wanted_words_combined,\n",
    "                                        testing_percentage=10,\n",
    "                                        validation_percentage=10\n",
    "                                        )\n",
    "\n",
    "index_to_word = {v:k for k,v in data_iter.word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mini_batch_size = 1\n",
    "data = data_iter.get_data(n_mini_batch_size, 0, 'training')\n",
    "labels = data['y']\n",
    "\n",
    "duplicates = data_iter.get_duplicates(labels, 0, 'training')\n",
    "assert np.any(labels == duplicates['y'])\n",
    "\n",
    "non_duplicates = data_iter.get_nonduplicates(labels, 0, 'training')\n",
    "assert np.any(labels != non_duplicates['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = np.random.randint(data['y'].shape[0])\n",
    "sample_idx = i\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15,10))\n",
    "\n",
    "# plt.figure()\n",
    "im = axes[0][0].imshow(data['x'][i].T)\n",
    "# fig.colorbar(im)\n",
    "label = index_to_word[data['y'][0]]\n",
    "axes[0][0].title.set_text(f'{label}')\n",
    "\n",
    "axes[0][1].imshow(duplicates['x'][i].T)\n",
    "# plt.colorbar()\n",
    "label = index_to_word[data['y'][0]]\n",
    "axes[0][1].title.set_text(f'{label}')\n",
    "\n",
    "\n",
    "axes[0][2].imshow(non_duplicates['x'][i].T)\n",
    "# plt.colorbar()\n",
    "label = index_to_word[non_duplicates['y'][0]]\n",
    "axes[0][2].title.set_text(f'{label}')\n",
    "\n",
    "######################################## DTW\n",
    "\n",
    "x = data['x'][i]\n",
    "\n",
    "# self dtw (extpected to be 0)\n",
    "\n",
    "y = data['x'][i]\n",
    "res = dtwalign.dtw(x, y, dist=DISTANCE, step_pattern=STEP_PATTERN, open_end=True)\n",
    "res.plot_path(ax=axes[1][0])\n",
    "axes[1][0].set_title(f\"anchor DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "\n",
    "# positive dtw\n",
    "\n",
    "y = duplicates['x'][sample_idx]\n",
    "res = dtwalign.dtw(x, y, dist=DISTANCE, step_pattern=STEP_PATTERN, open_end=True)\n",
    "res.plot_path(ax=axes[1][1])\n",
    "axes[1][1].set_title(f\"positive DTW = {res.normalized_distance:.4f}\")\n",
    "\n",
    "\n",
    "# negative dtw\n",
    "\n",
    "y = non_duplicates['x'][sample_idx]\n",
    "res = dtwalign.dtw(x, y, dist=DISTANCE, step_pattern=STEP_PATTERN, open_end=True)\n",
    "res.plot_path(ax=axes[1][2])\n",
    "axes[1][2].set_title(f\"negative DTW = {res.normalized_distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mini_batch_size = 1000\n",
    "n_duplicates_size= 100\n",
    "# gather data\n",
    "\n",
    "\n",
    "data = data_iter.get_data(n_mini_batch_size, 0, 'training')\n",
    "labels = data['y']\n",
    "\n",
    "duplicates = data_iter.get_duplicates(labels, 0, 'training')\n",
    "assert np.any(labels == duplicates['y'])\n",
    "duplicates['x'] = duplicates['x'][:n_duplicates_size]\n",
    "print(duplicates['x'].shape)\n",
    "\n",
    "non_duplicates = data_iter.get_nonduplicates(labels, 0, 'training')\n",
    "assert np.any(labels != non_duplicates['y'])\n",
    "\n",
    "y_true = np.concatenate((np.ones((n_duplicates_size)), np.zeros((n_mini_batch_size))))\n",
    "print(y_true)\n",
    "print(labels)\n",
    "print(sum(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MFCC Precision/Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize dist_lists\n",
    "\n",
    "duplicates_dtw = []\n",
    "non_duplicates_dtw = []\n",
    "\n",
    "for i in range(n_mini_batch_size):\n",
    "    print(i, end='\\r')\n",
    "    x = data['x'][i].squeeze()\n",
    "    \n",
    "    if i < n_duplicates_size :\n",
    "        y_duplicate = duplicates['x'][i].squeeze()\n",
    "        duplicates_dtw.append(dtwalign.dtw(x, y_duplicate, dist=DISTANCE, step_pattern=STEP_PATTERN, open_end=False, dist_only=True).normalized_distance)\n",
    "\n",
    "    y_non_duplicate = non_duplicates['x'][i].squeeze()\n",
    "    non_duplicates_dtw.append(dtwalign.dtw(x, y_non_duplicate, dist=DISTANCE, step_pattern=STEP_PATTERN, open_end=False, dist_only=True).normalized_distance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mfcc_dtw_dist = np.concatenate((duplicates_dtw, non_duplicates_dtw))\n",
    "precision, recall, _ = precision_recall_curve(y_true, -mfcc_dtw_dist)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.lineplot(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Improving DTW using latent variables from LSTM classifier as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nn\n",
    "\n",
    "nn_fname = r'C:/Study/SpeechAcquisitionModel/reports/seamise_net_10_03_2019_02_43_PM/net_0.484375.net'\n",
    "\n",
    "nn = torch.load(nn_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PREFIX_LENS = [50]\n",
    "\n",
    "for PREFIX_LEN in PREFIX_LENS:\n",
    "    print(f\"############################################################################\\n\\\n",
    "# PREFIX LENGTH : {PREFIX_LEN}\\n\\\n",
    "############################################################################\")\n",
    "    nn_input = torch.from_numpy(data['x'][:, :PREFIX_LEN, :]).cuda().float()\n",
    "\n",
    "\n",
    "    z, *_ = nn.single_forward(nn_input)\n",
    "    z = z.detach().cpu().numpy()\n",
    "\n",
    "    duplicates_z, *_ = nn.single_forward(torch.from_numpy(duplicates['x']).cuda().float())\n",
    "    duplicates_z = duplicates_z.detach().cpu().numpy()\n",
    "\n",
    "    non_duplicates_z, *_ = nn.single_forward(torch.from_numpy(non_duplicates['x']).cuda().float())\n",
    "    non_duplicates_z = non_duplicates_z.detach().cpu().numpy()\n",
    "\n",
    "    ##########################################################################\n",
    "    # OPEN END = False\n",
    "    ##########################################################################\n",
    "\n",
    "    # initialize dist_lists\n",
    "\n",
    "    duplicates_dtw_z = []\n",
    "    non_duplicates_dtw_z = []\n",
    "\n",
    "    duplicates_bmis = []\n",
    "    non_duplicates_bmis = []\n",
    "\n",
    "    for i in range(n_mini_batch_size):\n",
    "        print(i, end='\\r')\n",
    "        x = z[i].squeeze()\n",
    "        \n",
    "        if i < n_duplicates_size :\n",
    "            y_duplicate = duplicates_z[i].squeeze()\n",
    "            d = dtwalign.dtw(x, y_duplicate, dist=DISTANCE, step_pattern=CLOSED_END_STEP_PATTERN, open_end=False)\n",
    "            duplicates_dtw_z.append(d.normalized_distance)\n",
    "            duplicates_bmis.append(d.path[-1, -1])\n",
    "\n",
    "        y_non_duplicate = non_duplicates_z[i].squeeze()\n",
    "        d = dtwalign.dtw(x, y_non_duplicate, dist=DISTANCE, step_pattern=CLOSED_END_STEP_PATTERN, open_end=False)\n",
    "        non_duplicates_dtw_z.append(d.normalized_distance)\n",
    "        non_duplicates_bmis.append(d.path[-1, -1])\n",
    "    \n",
    "    ce_dtw_dist_ce = np.concatenate((duplicates_dtw_z, non_duplicates_dtw_z))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, -ce_dtw_dist_ce)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.lineplot(recall, precision)\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - SOFT-DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"############################################################################\\n\\\n",
    "# SOFT DTW\\n\\\n",
    "############################################################################\")\n",
    "\n",
    "PREFIX_LENS = [50]\n",
    "\n",
    "for PREFIX_LEN in PREFIX_LENS:\n",
    "    print(f\"############################################################################\\n\\\n",
    "# PREFIX LENGTH : {PREFIX_LEN}\\n\\\n",
    "############################################################################\")\n",
    "    nn_input = torch.from_numpy(data['x'][:, :PREFIX_LEN, :]).cuda().float()\n",
    "\n",
    "\n",
    "    z, *_ = nn.single_forward(nn_input)\n",
    "    z = z\n",
    "    z_detach = z.detach().cpu().numpy()\n",
    "\n",
    "    duplicates_z, *_ = nn.single_forward(torch.from_numpy(duplicates['x']).cuda().float())\n",
    "    duplicates_z = duplicates_z\n",
    "\n",
    "    non_duplicates_z, *_ = nn.single_forward(torch.from_numpy(non_duplicates['x']).cuda().float())\n",
    "    non_duplicates_z = non_duplicates_z\n",
    "\n",
    "    ###########################################################################\n",
    "    # DTW\n",
    "    ###########################################################################\n",
    "\n",
    "    soft_dtw_loss_open_end = SoftDTW(open_end=True, dist='l1')\n",
    "    soft_dtw_loss_close_end = SoftDTW(open_end=False, dist='l1')\n",
    "\n",
    "    ##########################################################################\n",
    "    # OPEN END = False\n",
    "    ##########################################################################\n",
    "\n",
    "    # initialize dist_lists\n",
    "\n",
    "    duplicates_dtw_z = []\n",
    "    non_duplicates_dtw_z = []\n",
    "\n",
    "    duplicates_bmis = []\n",
    "    non_duplicates_bmis = []\n",
    "\n",
    "    for i in range(n_mini_batch_size):\n",
    "        print(i, end='\\r')\n",
    "        x = z[i].squeeze()\n",
    "        \n",
    "        if i < n_duplicates_size :\n",
    "            y_duplicate = duplicates_z[i].squeeze()\n",
    "            d = soft_dtw_loss_close_end(x, y_duplicate)\n",
    "            duplicates_dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "        y_non_duplicate = non_duplicates_z[i].squeeze()\n",
    "        d = soft_dtw_loss_close_end(x, y_non_duplicate)\n",
    "        non_duplicates_dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "    ce_dtw_dist = np.concatenate((duplicates_dtw_z, non_duplicates_dtw_z))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, -ce_dtw_dist)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.lineplot(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Improving DTW using latent variables from L2-Triplet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nn\n",
    "nn_fname = r'C:/Study/SpeechAcquisitionModel/reports/seamise_net_10_03_2019_05_09_PM/net_0.421875.net'\n",
    "\n",
    "\n",
    "nn = torch.load(nn_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - SOFT-DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"############################################################################\\n\\\n",
    "# SOFT DTW\\n\\\n",
    "############################################################################\")\n",
    "\n",
    "PREFIX_LENS = [50]\n",
    "\n",
    "for PREFIX_LEN in PREFIX_LENS:\n",
    "    print(f\"############################################################################\\n\\\n",
    "# PREFIX LENGTH : {PREFIX_LEN}\\n\\\n",
    "############################################################################\")\n",
    "    nn_input = torch.from_numpy(data['x'][:, :PREFIX_LEN, :]).cuda().float()\n",
    "\n",
    "\n",
    "    z, *_ = nn.single_forward(nn_input)\n",
    "    z = z\n",
    "    z_detach = z.detach().cpu().numpy()\n",
    "\n",
    "    duplicates_z, *_ = nn.single_forward(torch.from_numpy(duplicates['x']).cuda().float())\n",
    "    duplicates_z = duplicates_z\n",
    "\n",
    "    non_duplicates_z, *_ = nn.single_forward(torch.from_numpy(non_duplicates['x']).cuda().float())\n",
    "    non_duplicates_z = non_duplicates_z\n",
    "\n",
    "    ###########################################################################\n",
    "    # DTW\n",
    "    ###########################################################################\n",
    "\n",
    "    soft_dtw_loss_open_end = SoftDTW(open_end=True, dist='l1')\n",
    "    soft_dtw_loss_close_end = SoftDTW(open_end=False, dist='l1')\n",
    "\n",
    "    ##########################################################################\n",
    "    # OPEN END = False\n",
    "    ##########################################################################\n",
    "\n",
    "    # initialize dist_lists\n",
    "\n",
    "    duplicates_dtw_z = []\n",
    "    non_duplicates_dtw_z = []\n",
    "\n",
    "    duplicates_bmis = []\n",
    "    non_duplicates_bmis = []\n",
    "\n",
    "    for i in range(n_mini_batch_size):\n",
    "        print(i, end='\\r')\n",
    "        x = z[i].squeeze()\n",
    "        \n",
    "        if i < n_duplicates_size :\n",
    "            y_duplicate = duplicates_z[i].squeeze()\n",
    "            d = soft_dtw_loss_close_end(x, y_duplicate)\n",
    "            duplicates_dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "        y_non_duplicate = non_duplicates_z[i].squeeze()\n",
    "        d = soft_dtw_loss_close_end(x, y_non_duplicate)\n",
    "        non_duplicates_dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "    ce_l2_dtw_dist = np.concatenate((duplicates_dtw_z, non_duplicates_dtw_z))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, -ce_l2_dtw_dist)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.lineplot(recall, precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Improving DTW using latent variables from DTW-Triplet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nn\n",
    "\n",
    "nn_fname = r'C:/Study/SpeechAcquisitionModel/reports/seamise_net_10_01_2019_03_37_PM/net_0.46875.net'\n",
    "\n",
    "nn = torch.load(nn_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - SOFT-DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"############################################################################\\n\\\n",
    "# SOFT DTW\\n\\\n",
    "############################################################################\")\n",
    "\n",
    "PREFIX_LENS = [50]\n",
    "\n",
    "for PREFIX_LEN in PREFIX_LENS:\n",
    "    print(f\"############################################################################\\n\\\n",
    "# PREFIX LENGTH : {PREFIX_LEN}\\n\\\n",
    "############################################################################\")\n",
    "    nn_input = torch.from_numpy(data['x'][:, :PREFIX_LEN, :]).cuda().float()\n",
    "\n",
    "\n",
    "    z, *_ = nn.single_forward(nn_input)\n",
    "    z = z\n",
    "    z_detach = z.detach().cpu().numpy()\n",
    "\n",
    "    duplicates_z, *_ = nn.single_forward(torch.from_numpy(duplicates['x']).cuda().float())\n",
    "    duplicates_z = duplicates_z\n",
    "\n",
    "    non_duplicates_z, *_ = nn.single_forward(torch.from_numpy(non_duplicates['x']).cuda().float())\n",
    "    non_duplicates_z = non_duplicates_z\n",
    "\n",
    "    ##########################################################################\n",
    "    # OPEN END = False\n",
    "    ##########################################################################\n",
    "\n",
    "    # initialize dist_lists\n",
    "\n",
    "    duplicates_dtw_z = []\n",
    "    non_duplicates_dtw_z = []\n",
    "\n",
    "    duplicates_bmis = []\n",
    "    non_duplicates_bmis = []\n",
    "\n",
    "    for i in range(n_mini_batch_size):\n",
    "        print(i, end='\\r')\n",
    "        x = z[i].squeeze()\n",
    "\n",
    "        if i < n_duplicates_size :\n",
    "            y_duplicate = duplicates_z[i].squeeze()\n",
    "            d = soft_dtw_loss_close_end(x, y_duplicate)\n",
    "            duplicates_dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "        y_non_duplicate = non_duplicates_z[i].squeeze()\n",
    "        d = soft_dtw_loss_close_end(x, y_non_duplicate)\n",
    "        non_duplicates_dtw_z.append(d.detach().cpu().numpy())\n",
    "\n",
    "    ce_dtw_dtw_dist = np.concatenate((duplicates_dtw_z, non_duplicates_dtw_z))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, -ce_dtw_dtw_dist)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.lineplot(recall, precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Precision_Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# # no skill model\n",
    "# precision, recall, _ = precision_recall_curve(y_true, np.zeros(len(y_true)), pos_label=1)\n",
    "# ax = sns.lineplot(recall, precision)\n",
    "# ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "\n",
    "models_dists = [mfcc_dtw_dist, ce_dtw_dist, ce_l2_dtw_dist, ce_dtw_dtw_dist]\n",
    "model_names = ['MFCC', 'CE', 'CE+L2-triplet', 'CE+DTW-triplet']\n",
    "for i in range(len(model_names)):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, -models_dists[i], pos_label=1)\n",
    "    print(thresholds)\n",
    "    sns.lineplot(recall, precision, label=model_names[i], linewidth=3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Average Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dists = [mfcc_dtw_dist, ce_dtw_dist, ce_l2_dtw_dist, ce_dtw_dtw_dist]\n",
    "model_names = ['MFCC', 'CE', 'CE+L2-triplet', 'CE+DTW-triplet']\n",
    "for i in range(len(model_names)):\n",
    "    average_precision = average_precision_score(y_true, -models_dists[i])\n",
    "    print(model_names[i], average_precision)\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, models_dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
