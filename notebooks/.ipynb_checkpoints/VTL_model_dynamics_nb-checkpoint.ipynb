{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s3628075\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import deque\n",
    "from random import randrange\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.initializers import RandomUniform\n",
    "from tensorflow.python.keras.layers import Dense, Input, BatchNormalization, Activation, Concatenate\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Study\\SpeechAcquisitionModel\")\n",
    "\n",
    "from src.VTL.vtl_environment import VTLEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dir = r\"C:\\Study\\SpeechAcquisitionModel\\reports\\videos\"\n",
    "dt = str(datetime.datetime.now().strftime(\"%m_%d_%Y_%I_%M_%p\"))\n",
    "os.makedirs(videos_dir + r'\\videos_' + dt)\n",
    "videos_dir = videos_dir + r'\\videos_' + dt\n",
    "summaries_dir = r\"C:\\Study\\SpeechAcquisitionModel\\reports\\summaries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s0, g0, a, s1, g1, target):\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append((s0, g0, a, s1, g1, target))\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append((s0, g0, a, s1, g1, target))\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s0_batch = np.array([_[0] for _ in batch])\n",
    "        g0_batch = np.array([_[1] for _ in batch])\n",
    "        a_batch = np.array([_[2] for _ in batch])\n",
    "        s1_batch = np.array([_[3] for _ in batch])\n",
    "        g1_batch = np.array([_[4] for _ in batch])\n",
    "        target_batch = np.array([_[5] for _ in batch])\n",
    "\n",
    "        return s0_batch, g0_batch, a_batch, s1_batch, g1_batch, target_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.01, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDynamics(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the current state and the goal state, output is the next state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, model_settings):\n",
    "        self.sess = tf.get_default_session()\n",
    "        assert (self.sess is not None)\n",
    "        self.name = name\n",
    "        self.s_dim = model_settings['state_dim']\n",
    "        self.state_bound = model_settings['state_bound']\n",
    "        self.g_dim = model_settings['goal_dim']\n",
    "        self.goal_bound = model_settings['goal_bound']\n",
    "        self.a_dim = model_settings['action_dim']\n",
    "        self.action_bound = model_settings['action_bound']\n",
    "        self.learning_rate = model_settings['actor_learning_rate']\n",
    "        self.tau = model_settings['actor_tau']\n",
    "        self.batch_size = model_settings['minibatch_size']\n",
    "\n",
    "        y_max = [y[1] for y in self.state_bound]\n",
    "        y_min = [y[0] for y in self.state_bound]\n",
    "        self._k_state = 2. / (np.subtract(y_max, y_min))\n",
    "        self._b_state = -0.5 * np.add(y_max, y_min) * self._k_state\n",
    "\n",
    "        y_max = [y[1] for y in self.goal_bound]\n",
    "        y_min = [y[0] for y in self.goal_bound]\n",
    "        self._k_goal = 2. / (np.subtract(y_max, y_min))\n",
    "        self._b_goal = -0.5 * np.add(y_max, y_min) * self._k_goal\n",
    "\n",
    "        # Model Dynamics Network\n",
    "        with tf.variable_scope(self.name + '_model_dynamics'):\n",
    "            self.inputs_state,\\\n",
    "            self.inputs_goal,\\\n",
    "            self.inputs_action,\\\n",
    "            self.state_out,\\\n",
    "            self.scaled_state_out,\\\n",
    "            self.goal_out,\\\n",
    "            self.scaled_goal_out\\\n",
    "                = self.create_model_dynamics_network()\n",
    "            self.network_params = tf.trainable_variables(scope=self.name + '_model_dynamics')\n",
    "\n",
    "        # Target Model Dynamics Network\n",
    "        with tf.variable_scope(self.name + '_target_model_dynamics'):\n",
    "            self.target_inputs_state,\\\n",
    "            self.target_inputs_goal,\\\n",
    "            self.target_inputs_action,\\\n",
    "            self.target_state_out,\\\n",
    "            self.target_scaled_state_out,\\\n",
    "            self.target_goal_out,\\\n",
    "            self.target_scaled_goal_out\\\n",
    "                = self.create_model_dynamics_network()\n",
    "            self.target_network_params = tf.trainable_variables(scope=self.name + '_target_model_dynamics')\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_network_params))]\n",
    "\n",
    "        self.copy_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "                                    for i in range(len(self.target_network_params))]\n",
    "\n",
    "        self.ground_truth_state_out = tf.placeholder(tf.float32, [None, self.s_dim])\n",
    "        self.ground_truth_goal_out = tf.placeholder(tf.float32, [None, self.g_dim])\n",
    "#         self.ground_truth_out = Concatenate()([self.ground_truth_state_out, self.ground_truth_goal_out])\n",
    "        self.ground_truth_out = self.ground_truth_state_out\n",
    "\n",
    "        self.scaled_out = self.scaled_state_out\n",
    "#         self.scaled_out = Concatenate()([self.scaled_state_out, self.scaled_goal_out])\n",
    "\n",
    "        # Optimization Op\n",
    "        self.loss = tf.losses.mean_squared_error(self.ground_truth_out, self.scaled_out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Acion gradients extraction\n",
    "        self.goal_loss = tf.losses.mean_squared_error(self.ground_truth_goal_out, self.scaled_goal_out)\n",
    "        # self.actor_obj = tf.abs(tf.subtract(self.ground_truth_goal_out, self.scaled_goal_out))\n",
    "        self.action_grads = tf.gradients(self.goal_loss, self.inputs_action)\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_model_dynamics_network(self):\n",
    "        state_x = Input(batch_shape=[None, self.s_dim])\n",
    "        goal_x = Input(batch_shape=[None, self.g_dim])\n",
    "        action_x = Input(batch_shape=[None, self.a_dim])\n",
    "\n",
    "        state_net = Dense(256)(state_x)\n",
    "        state_net = BatchNormalization()(state_net)\n",
    "        state_net = Activation('relu')(state_net)\n",
    "\n",
    "        goal_net = Dense(256)(goal_x)\n",
    "        goal_net = BatchNormalization()(goal_net)\n",
    "        goal_net = Activation('relu')(goal_net)\n",
    "\n",
    "        net = Concatenate()([state_net, goal_net])\n",
    "        net  = state_net\n",
    "        net = Dense(128, activation='relu')(net)\n",
    "\n",
    "        action_net = Dense(128, activation='relu')(action_x)\n",
    "\n",
    "        net = Concatenate()([net, action_net])\n",
    "        net = Dense(64)(net)\n",
    "        net = BatchNormalization()(net)\n",
    "        net = Activation('tanh')(net)\n",
    "\n",
    "        # state output branch\n",
    "        state_y = Dense(self.s_dim,\n",
    "                        activation='tanh',\n",
    "                        kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003)\n",
    "                        )(net)\n",
    "        state_y = Dense(self.s_dim)(state_y)\n",
    "        state_y_scaled = state_y\n",
    "        # state_y_scaled = tf.subtract(state_y, self._b_state)\n",
    "        # state_y_scaled = tf.divide(state_y_scaled, self._k_state)\n",
    "\n",
    "        # goal output branch\n",
    "        goal_y = Dense(self.g_dim,\n",
    "                        activation='tanh',\n",
    "                        kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003)\n",
    "                        )(net)\n",
    "        goal_y = Dense(self.g_dim)(goal_y)\n",
    "\n",
    "        goal_y_scaled = goal_y\n",
    "        # goal_y_scaled = tf.subtract(goal_y, self._b_goal)\n",
    "        # goal_y_scaled = tf.divide(goal_y_scaled, self._k_goal)\n",
    "        return state_x, goal_x, action_x, state_y, state_y_scaled, goal_y, goal_y_scaled\n",
    "\n",
    "    def train(self, inputs_state, inputs_goal, inputs_action, ground_truth_state_out, ground_truth_goal_out):\n",
    "        return self.sess.run([self.optimize, self.loss, self.goal_loss], feed_dict={\n",
    "                self.inputs_state: inputs_state,\n",
    "                self.inputs_action: inputs_action,\n",
    "                self.inputs_goal: inputs_goal,\n",
    "                self.ground_truth_state_out: ground_truth_state_out,\n",
    "                self.ground_truth_goal_out: ground_truth_goal_out,\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs_state, inputs_goal, inputs_action, target_goal_out):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_action: inputs_action,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.ground_truth_goal_out: target_goal_out\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs_state, inputs_goal, inputs_action):\n",
    "        return self.sess.run([self.scaled_state_out, self.scaled_goal_out], feed_dict={\n",
    "            self.inputs_state: inputs_state,\n",
    "            self.inputs_goal: inputs_goal,\n",
    "            self.inputs_action: inputs_action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs_state, inputs_goal, inputs_action):\n",
    "        return self.sess.run([self.target_scaled_state_out, self.target_scaled_goal_out], feed_dict={\n",
    "            self.target_inputs_state: inputs_state,\n",
    "            self.target_inputs_goal: inputs_goal,\n",
    "            self.target_inputs_action: inputs_action\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def init_target_network(self):\n",
    "        self.sess.run(self.copy_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "def build_summaries():\n",
    "    model_dynamics_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Model dynamics loss\", model_dynamics_loss)\n",
    "\n",
    "    model_dynamics_relative_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Model dynamics relative loss\", model_dynamics_relative_loss)\n",
    "\n",
    "    model_dynamics_goal_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Model dynamics goal loss\", model_dynamics_goal_loss)\n",
    "\n",
    "    actor_grads = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "    tf.summary.histogram('actor_gradients', actor_grads)\n",
    "\n",
    "    variables = [v for v in tf.trainable_variables()]\n",
    "    [tf.summary.histogram(v.name, v) for v in variables]\n",
    "\n",
    "    summary_vars = [model_dynamics_loss, model_dynamics_relative_loss, model_dynamics_goal_loss ,actor_grads]\n",
    "\n",
    "    summary_vars.extend(variables)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "\n",
    "class real_dynamics(object):\n",
    "    def __init__(self, settings):\n",
    "        with tf.variable_scope('real_model_dynamics'):\n",
    "            self.action_x = Input(batch_shape=[None, settings['action_dim']])\n",
    "            self.state_x = Input(batch_shape=[None, settings['state_dim']])\n",
    "            self.ground_truth_goal_out = tf.placeholder(tf.float32, [None, settings['state_dim']])\n",
    "            self.next_state = tf.add(self.state_x, self.action_x)\n",
    "            y_max = np.tile([y[1] for y in settings['state_bound']], (settings['minibatch_size'], 1))\n",
    "            y_min = np.tile([y[0] for y in settings['state_bound']], (settings['minibatch_size'], 1))\n",
    "            self.next_state = tf.clip_by_value(self.next_state, y_min, y_max)\n",
    "            self.goal_loss = tf.losses.mean_squared_error(self.ground_truth_goal_out, self.next_state)\n",
    "\n",
    "            # self.actor_obj = tf.abs(tf.subtract(self.ground_truth_goal_out, self.scaled_goal_out))\n",
    "            # self.action_grads = tf.gradients(self.goal_loss, self.action_x)\n",
    "            self.action_grads = \\\n",
    "                tf.gradients(self.goal_loss, self.action_x)\n",
    "\n",
    "    def action_gradients(self, action, state, ground_truth):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.action_grads, self.goal_loss], feed_dict={\n",
    "            self.action_x: action,\n",
    "            self.state_x: state,\n",
    "            self.ground_truth_goal_out: ground_truth\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(settings, model_dynamics, env, replay_buffer, reference_trajectory, noise_sigma):\n",
    "\n",
    "    # temp\n",
    "    dm = real_dynamics(settings)\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    writer = tf.summary.FileWriter(settings['summary_dir'] + '/summary_' + dt, sess.graph)\n",
    "\n",
    "\n",
    "    num_episodes = 10000\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(env.action_dim), sigma=noise_sigma)\n",
    "    s_dim = settings['state_dim']\n",
    "    g_dim = settings['goal_dim']\n",
    "    a_dim = settings['action_dim']\n",
    "    for i in range(num_episodes):\n",
    "        # pick random initial state from the reference trajectory\n",
    "        s0_index = randrange(0, reference_trajectory.shape[0] - 1)\n",
    "        if i % 200 == 0:\n",
    "            s0_index = 0\n",
    "\n",
    "        s0 = reference_trajectory[s0_index]\n",
    "        g0 = s0\n",
    "        env.reset(s0)\n",
    "\n",
    "        r = []\n",
    "        # rollout episode\n",
    "        for j in range(s0_index, len(reference_trajectory) - 1):\n",
    "            target = reference_trajectory[j + 1]\n",
    "            # add noise\n",
    "            action = action_noise()\n",
    "\n",
    "            action = np.reshape(action, (a_dim))\n",
    "            s1 = env.step(action)\n",
    "            env.render()\n",
    "            g1 = s1\n",
    "\n",
    "            s1_expected = s0 + action\n",
    "            err = np.mean(s1_expected - s1)\n",
    "            # calc reward\n",
    "            last_loss = np.linalg.norm(target - g1)\n",
    "\n",
    "            r.append( -1. * np.linalg.norm(target - g1))\n",
    "            replay_buffer.add(s0, g0, action, s1, g1, target)\n",
    "            s0 = s1\n",
    "            g0 = g1\n",
    "\n",
    "            if last_loss > 4. and i % 200 != 0:\n",
    "                break\n",
    "        if i % 200 == 0:\n",
    "            \n",
    "            fname = videos_dir + '/episode_' + str(datetime.datetime.now().strftime(\"%m_%d_%Y_%I_%M_%p_%S\"))\n",
    "            env.dump_episode(fname)\n",
    "        # train model_dynamics and policy\n",
    "        minibatch_size = settings['minibatch_size']\n",
    "        if replay_buffer.size() > minibatch_size:\n",
    "            s0_batch, g0_batch, a_batch, s1_batch, g1_batch, target_batch = \\\n",
    "                replay_buffer.sample_batch(minibatch_size)\n",
    "\n",
    "            # train model_dynamics\n",
    "            ds = s1_batch - s0_batch\n",
    "            dg = g1_batch - g0_batch\n",
    "            \n",
    "            md_loss = 0\n",
    "            md_goal_loss = 0\n",
    "#             _, md_loss, md_goal_loss = model_dynamics.train(s0_batch, g0_batch, a_batch, ds, dg)\n",
    "            \n",
    "            ds_pred, dg_pred = model_dynamics.predict(s0_batch, g0_batch, a_batch)\n",
    "            \n",
    "            md_loss = np.mean(np.linalg.norm(ds - ds_pred, axis = 1))\n",
    "            relative_loss = md_loss / np.mean(np.linalg.norm(ds, axis = 1))\n",
    "            if i % 200 == 0:\n",
    "                ds_pred, dg_pred = model_dynamics.predict(s0_batch, g0_batch, a_batch)\n",
    "                print(ds_pred[0] - ds[0])\n",
    "                print(dg_pred[0] - dg[0])\n",
    "\n",
    "            summary_str = sess.run(summary_ops, feed_dict={\n",
    "                summary_vars[0]: md_loss,\n",
    "                summary_vars[1]: relative_loss,\n",
    "                summary_vars[2]: md_goal_loss,\n",
    "                summary_vars[3]: 0\n",
    "            })\n",
    "\n",
    "            writer.add_summary(summary_str, i)\n",
    "            writer.flush()\n",
    "\n",
    "            print(' Episode: {:d} |'\n",
    "                  ' Model dynamics loss: {:.4f}|'\n",
    "                  ' Model dynamics relative loss: {:.4f}|'\n",
    "                  ' MD goal loss: {:.4f}'.format(i,\n",
    "                                                 md_loss,\n",
    "                                                 relative_loss,\n",
    "                                                 md_goal_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile date of the library: \"Jun 15 2018\"\n",
      "INFO:tensorflow:Summary name Model dynamics loss is illegal; using Model_dynamics_loss instead.\n",
      "INFO:tensorflow:Summary name Model dynamics relative loss is illegal; using Model_dynamics_relative_loss instead.\n",
      "INFO:tensorflow:Summary name Model dynamics goal loss is illegal; using Model_dynamics_goal_loss instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense/kernel:0 is illegal; using MD1_model_dynamics/dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense/bias:0 is illegal; using MD1_model_dynamics/dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/batch_normalization/gamma:0 is illegal; using MD1_model_dynamics/batch_normalization/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/batch_normalization/beta:0 is illegal; using MD1_model_dynamics/batch_normalization/beta_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_1/kernel:0 is illegal; using MD1_model_dynamics/dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_1/bias:0 is illegal; using MD1_model_dynamics/dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/batch_normalization_1/gamma:0 is illegal; using MD1_model_dynamics/batch_normalization_1/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/batch_normalization_1/beta:0 is illegal; using MD1_model_dynamics/batch_normalization_1/beta_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_2/kernel:0 is illegal; using MD1_model_dynamics/dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_2/bias:0 is illegal; using MD1_model_dynamics/dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_3/kernel:0 is illegal; using MD1_model_dynamics/dense_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_3/bias:0 is illegal; using MD1_model_dynamics/dense_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_4/kernel:0 is illegal; using MD1_model_dynamics/dense_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_4/bias:0 is illegal; using MD1_model_dynamics/dense_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/batch_normalization_2/gamma:0 is illegal; using MD1_model_dynamics/batch_normalization_2/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/batch_normalization_2/beta:0 is illegal; using MD1_model_dynamics/batch_normalization_2/beta_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_5/kernel:0 is illegal; using MD1_model_dynamics/dense_5/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_5/bias:0 is illegal; using MD1_model_dynamics/dense_5/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_6/kernel:0 is illegal; using MD1_model_dynamics/dense_6/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_6/bias:0 is illegal; using MD1_model_dynamics/dense_6/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_7/kernel:0 is illegal; using MD1_model_dynamics/dense_7/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_7/bias:0 is illegal; using MD1_model_dynamics/dense_7/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_8/kernel:0 is illegal; using MD1_model_dynamics/dense_8/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_model_dynamics/dense_8/bias:0 is illegal; using MD1_model_dynamics/dense_8/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense/kernel:0 is illegal; using MD1_target_model_dynamics/dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense/bias:0 is illegal; using MD1_target_model_dynamics/dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/batch_normalization/gamma:0 is illegal; using MD1_target_model_dynamics/batch_normalization/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/batch_normalization/beta:0 is illegal; using MD1_target_model_dynamics/batch_normalization/beta_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_1/kernel:0 is illegal; using MD1_target_model_dynamics/dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_1/bias:0 is illegal; using MD1_target_model_dynamics/dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/batch_normalization_1/gamma:0 is illegal; using MD1_target_model_dynamics/batch_normalization_1/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/batch_normalization_1/beta:0 is illegal; using MD1_target_model_dynamics/batch_normalization_1/beta_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_2/kernel:0 is illegal; using MD1_target_model_dynamics/dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_2/bias:0 is illegal; using MD1_target_model_dynamics/dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_3/kernel:0 is illegal; using MD1_target_model_dynamics/dense_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_3/bias:0 is illegal; using MD1_target_model_dynamics/dense_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_4/kernel:0 is illegal; using MD1_target_model_dynamics/dense_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_4/bias:0 is illegal; using MD1_target_model_dynamics/dense_4/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/batch_normalization_2/gamma:0 is illegal; using MD1_target_model_dynamics/batch_normalization_2/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/batch_normalization_2/beta:0 is illegal; using MD1_target_model_dynamics/batch_normalization_2/beta_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_5/kernel:0 is illegal; using MD1_target_model_dynamics/dense_5/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_5/bias:0 is illegal; using MD1_target_model_dynamics/dense_5/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_6/kernel:0 is illegal; using MD1_target_model_dynamics/dense_6/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_6/bias:0 is illegal; using MD1_target_model_dynamics/dense_6/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_7/kernel:0 is illegal; using MD1_target_model_dynamics/dense_7/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_7/bias:0 is illegal; using MD1_target_model_dynamics/dense_7/bias_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_8/kernel:0 is illegal; using MD1_target_model_dynamics/dense_8/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name MD1_target_model_dynamics/dense_8/bias:0 is illegal; using MD1_target_model_dynamics/dense_8/bias_0 instead.\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitrate tolerance 128000 too small for bitrate 8000000, overriding\n",
      "Warning: data is not aligned! This can lead to a speedloss\n",
      "Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5c46490e6197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtarget_trajectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtract_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglottis_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_trajectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_sigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-256a4eebea81>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(settings, model_dynamics, env, replay_buffer, reference_trajectory, noise_sigma)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0ms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Study\\SpeechAcquisitionModel\\src\\VTL\\vtl_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, render)\u001b[0m\n\u001b[0;32m    124\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudio_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m                          )\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "speaker_fname = os.path.join(r'C:\\Study\\SpeechAcquisitionModel\\src\\VTL', 'JD2.speaker')\n",
    "lib_path = os.path.join(r'C:\\Study\\SpeechAcquisitionModel\\src\\VTL', 'VocalTractLab2.dll')\n",
    "ep_duration = 5000\n",
    "timestep = 20\n",
    "env = VTLEnv(lib_path, speaker_fname, timestep, max_episode_duration=ep_duration)\n",
    "\n",
    "settings = {\n",
    "        'state_dim': env.state_dim,\n",
    "        'action_dim': env.action_dim,\n",
    "        'state_bound': env.state_bound,\n",
    "        'action_bound': env.action_bound,\n",
    "        'goal_dim': env.state_dim,\n",
    "        'goal_bound': env.state_bound,\n",
    "        'episode_length': 40,\n",
    "        'minibatch_size': 512,\n",
    "\n",
    "        'actor_tau': 0.01,\n",
    "        'actor_learning_rate': 0.0001,\n",
    "\n",
    "        'summary_dir': summaries_dir\n",
    "    }\n",
    "noise_sigma = 0.01\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    md = ModelDynamics('MD1', settings)\n",
    "    replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    reference_fname = r'C:\\Study\\SpeechAcquisitionModel\\src\\VTL\\references\\a_i.pkl'\n",
    "    with open(reference_fname, 'rb') as f:\n",
    "        (tract_params, glottis_params) = pickle.load(f)\n",
    "        target_trajectory = np.hstack((np.array(tract_params), np.array(glottis_params)))\n",
    "    \n",
    "    train(settings, md, env, replay_buffer, target_trajectory, noise_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
